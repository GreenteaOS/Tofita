// The Tofita Kernel
// Copyright (C) 2020  Oleg Petrenko
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Lesser General Public License as published by
// the Free Software Foundation, version 3 of the License.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Lesser General Public License for more details.
//
// You should have received a copy of the GNU Lesser General Public License
// along with this program.  If not, see <https://www.gnu.org/licenses/>.

// AMD64 pages format and map/unmap internal implementation

let addressBits = 12

@struct @packed @sizeOf(8)
class LinearAddress {
	UInt16 offset : 12
	UInt16 pt : 9
	UInt16 pd : 9
	UInt16 pdpt : 9
	UInt16 pml4 : 9
	UInt16 reserved : 16
}

// 512 entries * 8 bytes = 4 KiB
// PML4 size = PDP size = PD size = PT size
//
// Since we are using 64 bit mode, the entry size
// is 64 bits.

let pageTableSize = 512

// TODO remove this
@aligned(4096) var pml4entries: Pointer<PageEntry> = null

LinearAddress getLinearAddress(virtualAddr: UInt64) {
	return *((LinearAddress *)Pointer.of(virtualAddr))
}

fun initializePage(entry: Pointer<PageEntry>, address: UInt64) {
	entry.address = address >> addressBits
	entry.present = 1
	entry.writeAllowed = 1

	// TODO
	entry.accessibleByAll = 1
}

fun getPage(table: Pointer<PageEntry>, entryId: UInt64): Pointer<PageEntry> {
	var entry: Pointer<PageEntry> = Pointer.of(table[entryId])

	if (entry.present == 1) {
		return ((entry.address << addressBits) + (wholePhysicalStart as! UInt64)) as! Pointer<PageEntry>
	} else {
		var newPage = PhysicalAllocator.allocateOnePagePreZeroed() as! Pointer<PageEntry>
		if (newPage != null)
			initializePage(entry, (newPage as! UInt64) - (wholePhysicalStart as! UInt64))
		return newPage
	}
}

fun map_pt(PageEntry pt[], virtualAddr: UInt64, physicalAddr: UInt64) {
	var entry: Pointer<PageEntry> = Pointer.of(pt[getLinearAddress(virtualAddr).pt])
	initializePage(entry, physicalAddr)
}

// TODO handle huge pages returned from bootloader
#define createMapping(fromTable, toTable)                                                                    \
	Void map_##fromTable(PageEntry fromTable[], virtualAddr: UInt64, physicalAddr: UInt64) {               \
		var toTable: Pointer<PageEntry> = getPage(fromTable, getLinearAddress(virtualAddr).fromTable);                    \
		map_##toTable(toTable, virtualAddr, physicalAddr);                                                   \
	}

createMapping(pd, pt)
createMapping(pdpt, pd)
createMapping(pml4, pdpt)

// Returns actual physical address from virtual address
fun resolveAddr(const pml4entries: Pointer<PageEntry>, virtualAddr: UInt64): UInt64 {
	var linear = getLinearAddress(virtualAddr)
	// pml4
	const PageEntry *entry = Pointer.of(pml4entries[linear.pml4])
	if (entry.present == 0)
		return PHYSICAL_NOT_FOUND
	entry = (const PageEntry *)((entry.address << addressBits) + (wholePhysicalStart as! UInt64))
	// pdpt
	entry = Pointer.of(entry[linear.pdpt])
	if (entry.present == 0)
		return PHYSICAL_NOT_FOUND
	// TODO handle mega pages
	entry = (const PageEntry *)((entry.address << addressBits) + (wholePhysicalStart as! UInt64))
	// pd
	entry = Pointer.of(entry[linear.pd])
	if (entry.present == 0)
		return PHYSICAL_NOT_FOUND
	if (entry.largePage == 1)
		return (entry.address << addressBits) + linear.pt * 4096 + linear.offset
	entry = (const PageEntry *)((entry.address << addressBits) + (wholePhysicalStart as! UInt64))
	// pt
	entry = Pointer.of(entry[linear.pt])
	if (entry.present == 0)
		return PHYSICAL_NOT_FOUND
	return (entry.address << addressBits) + linear.offset
}

// TODO return error code (as enum)
// TODO ^ hexa @mustCheckReturn for return values (like Golang)
// TODO ^ same clang warning
// TODO should accept usermode flag so shared memory and GDI buffers can be mapped out of user scope
// ^ probably should be other way for WoW processes
fun mapMemory(pml4entries: Pointer<PageEntry>, virtualAddr: UInt64, physicalAddr: UInt64, pageCount: UInt32) {
	serialPrintln("[paging] mapping memory range")

	var virtualAddrEnd: UInt64 = virtualAddr + pageCount * pageSize

	var vAddress: UInt64 = virtualAddr
	var pAddress: UInt64 = physicalAddr

	serialPrintf("[paging.range] bytes = %d or %d\n", virtualAddrEnd - virtualAddr, pageCount * pageSize)

	serialPrint("[paging.range] virtual address = ")
	serialPrintHex((uint64_t)(virtualAddr))
	serialPrint("\n")

	serialPrint("[paging.range] physical address = ")
	serialPrintHex((uint64_t)(physicalAddr))
	serialPrint("\n")

	serialPrint("[paging.range] page count = ")
	serialPrintHex((uint64_t)(pageCount))
	serialPrint("\n")

	serialPrint("[paging.range] virtual address end = ")
	serialPrintHex((uint64_t)(virtualAddrEnd))
	serialPrint("\n")

	while (vAddress < virtualAddrEnd) {
		map_pml4(pml4entries, vAddress, pAddress)

		vAddress += pageSize
		pAddress += pageSize
	}

	// TODO rethink where to call this
	amd64.writeCr3((pml4entries as! UInt64) - (wholePhysicalStart as! UInt64))
}

// Unmaps pages without actually deallocating them, still deallocates unused page tables
// (page tables are managed only and only here anyway)
fun unmapMemory(pml4entries: Pointer<PageEntry>, virtualAddr: UInt64, pageCount: UInt32) {
	// TODO do NOT unmap reserved pages, i.e. wholePhysicalStart and upper half region!
	// TODO probably special shared buffer allocator+mapper which uses linked list
	// way upper than wholePhysicalStart limit
	// TODO
}

// Completely unmaps and deallocates pages and page tables
fun unmapAndFreeMemory(pml4entries: Pointer<PageEntry>, virtualAddr: UInt64, pageCount: UInt32) {
	// TODO resolve and dealloc pages
	unmapMemory(pml4entries, virtualAddr, pageCount)
}

// Upper half
// Just makes a copy of upper half's PML4,
// because it is always the same between processes
fun copyKernelMemoryMap(const pml4source: Pointer<PageEntry>, pml4destination: Pointer<PageEntry>) {
	var i: UInt32 = 255 // TODO validate that this is where upper half starts
	while (i < pageTableSize) {
		pml4destination[i] = pml4source[i]
		i++
	}
}

// Creates new PML4 for new process
fun newCR3(const pml4source: Pointer<PageEntry>): Pointer<PageEntry> {
	var pml4result: Pointer<PageEntry> = PhysicalAllocator.allocateOnePagePreZeroed() as! Pointer<PageEntry>
	if (pml4result != null)
		copyKernelMemoryMap(pml4source, pml4result)
	return pml4result
}

fun freeCR3(pml4: Pointer<PageEntry>): Pointer<PageEntry> {
	// TODO deallocate full lower half
}

// Lower half
// TODO protection ring
// TODO lower half limit bounds check (it is less than upper starting range)
fun mapUserspaceMemory(pml4entries: Pointer<PageEntry>, virtualAddr: UInt64, physicalAddr: UInt64,
							pageCount: UInt32) {}

// Same as VirtualAlloc
// TODO protection ring
// TODO respect WoW limits
// TODO VirtualAllocEx-like behavior (i.e. alloc for other processes, not only current)
// Decides automatically where to allocate
// if virtualAddr == 0
// Note: also allocates physical pages, i.e consumes extra memory
fun allocUserspaceMemory(pml4entries: Pointer<PageEntry>, virtualAddr: UInt64, pageCount: UInt32) {}
