// The Tofita Kernel
// Copyright (C) 2022 Oleg Petrenko
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Lesser General Public License as published by
// the Free Software Foundation, version 3 of the License.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Lesser General Public License for more details.
//
// You should have received a copy of the GNU Lesser General Public License
// along with this program.  If not, see <https://www.gnu.org/licenses/>.

let addressBits = 12

@struct @packed @sizeOf(8)
class LinearAddress {
	unsigned long long offset : 12
	unsigned long long pt : 9
	unsigned long long pd : 9
	unsigned long long pdpt : 9
	unsigned long long pml4 : 9
	unsigned long long reserved : 16
}

// 512 entries * 8 bytes = 4 KiB
// PML4 size = PDP size = PD size = PT size
//
// Since we are using 64 bit mode, the entry size
// is 64 bits.

let pageTableSize = 512

// Entry in a page table
@struct @packed @sizeOf(8)
class PageEntry {
	// Is the page present in physical memory?
	unsigned long long present : 1

	// Pages are read-only by default
	unsigned long long writeAllowed : 1

	// Pages are only accessible by supervisor by default
	unsigned long long accessibleByAll : 1

	// Write through abilities of the page
	unsigned long long writeThrough : 1

	// If set, the page will not be cached
	unsigned long long cacheDisabled : 1

	// Was the page accessed?
	unsigned long long accessed : 1

	// Has the page been written to? Only applicable for PTE.
	unsigned long long dirty : 1

	// Page size by default is small, enabling this bit makes it bigger. Only applicable for PTE
	unsigned long long largePage : 1

	// Prevent the translations cache from updating
	// the address in cache if CR3 is reset.
	//
	// Page global enable bit in CR4 has to be set to
	// enable this feature.
	//
	// Only applicable for PTE
	unsigned long long global : 1

	// Not used by the processor
	unsigned long long metadata : 3

	// Physical address of the child table/page
	unsigned long long address : 40

	// Not used by the processor
	unsigned long long metadata2 : 7

	// Only applicable for PTE
	unsigned long long protectionKey : 4

	// Disable execution of code from this page
	unsigned long long disableExecution : 1
}

var pml4entries: Pointer<PageEntry> = null

Void *allocatePage() {
	// TODO bounds check
	return (Void *)conventionalAllocateNext(pageSize)
}

LinearAddress getLinearAddress(address: UInt64) {
	return *((LinearAddress *)Pointer.of(address))
}

fun initializePage(entry: Pointer<PageEntry>, address: UInt64, writeAllowed: UInt8) {
	entry.address = address >> addressBits
	entry.present = 1
	entry.writeAllowed = (writeAllowed == 1) ? 1 : 0
	entry.accessibleByAll = 1 // TODO
}

fun initializePageHuge(entry: Pointer<PageEntry>, address: UInt64) {
	entry.address = address >> addressBits
	entry.present = 1
	entry.writeAllowed = 1
	entry.largePage = 1
	entry.accessibleByAll = 1 // TODO
}

Void *getPage(table: Pointer<PageEntry>, entryId: UInt64, writeAllowed: UInt8) {
	var entry: Pointer<PageEntry> = Pointer.of(table[entryId])

	if (entry.present == 1) {
		return (Void *)(entry.address << addressBits)
	} else {
		var newPage: Pointer<Void> = allocatePage()
		initializePage(entry, (newPage as! UInt64), writeAllowed)
		return newPage
	}
}

fun map_pt(PageEntry pt[], virtualAddr: UInt64, physicalAddr: UInt64, writeAllowed: UInt8) {
	var entry: Pointer<PageEntry> = Pointer.of(pt[getLinearAddress(virtualAddr).pt])
	initializePage(entry, physicalAddr, writeAllowed)
}

fun map_p2huge(PageEntry pd[], virtualAddr: UInt64, physicalAddr: UInt64) {
	var entry: Pointer<PageEntry> = Pointer.of(pd[getLinearAddress(virtualAddr).pd])
	initializePageHuge(entry, physicalAddr)
}

#define createMapping(fromTable, toTable)                                                                    \
	Void map_##fromTable(PageEntry fromTable[], virtualAddr: UInt64, physicalAddr: UInt64,          \
								UInt8 writeAllowed) {                                                      \
		var toTable: Pointer<Void> = getPage(fromTable, getLinearAddress(virtualAddr).fromTable, writeAllowed)           \
		map_##toTable((PageEntry *)toTable, virtualAddr, physicalAddr, writeAllowed)                        \
	}

createMapping(pd, pt)
createMapping(pdpt, pd)
createMapping(pml4, pdpt)

#define createHugeMapping(name, fromTable, calls, toTable)                                                   \
	Void map_##name(PageEntry fromTable[], virtualAddr: UInt64, physicalAddr: UInt64) {             \
		var toTable: Pointer<Void> = getPage(fromTable, getLinearAddress(virtualAddr).fromTable, 1)                      \
		map_##calls((PageEntry *)toTable, virtualAddr, physicalAddr)                                        \
	}

createHugeMapping(p3huge, pdpt, p2huge, pd)
createHugeMapping(p4huge, pml4, p3huge, pdpt)

fun mapMemory(UInt64 virtualAddr, physicalAddr: UInt64, pageCount: UInt32, writeAllowed: UInt8) {
	serialPrintln("[paging] mapping memory range")

	var virtualAddrEnd: UInt64 = virtualAddr + pageCount * pageSize

	var vAddress: UInt64 = virtualAddr
	var pAddress: UInt64 = physicalAddr

	serialPrintf("[paging.range] bytes = %d or %d\n", virtualAddrEnd - virtualAddr, pageCount * pageSize)

	serialPrint("[paging.range] virtual address = ")
	serialPrintHex(virtualAddr as! UInt64)
	serialPrint("\n")

	serialPrint("[paging.range] physical address = ")
	serialPrintHex(physicalAddr as! UInt64))
	serialPrint("\n")

	serialPrint("[paging.range] page count = ")
	serialPrintHex(pageCount as! UInt64)
	serialPrint("\n")

	serialPrint("[paging.range] virtual address end = ")
	serialPrintHex(virtualAddrEnd as! UInt64)
	serialPrint("\n")

	while (vAddress < virtualAddrEnd) {
		map_pml4(pml4entries, vAddress, pAddress, writeAllowed)

		vAddress += pageSize
		pAddress += pageSize
	}
}

fun mapMemoryHuge(virtualAddr: UInt64, physicalAddr: UInt64, pageCount: UInt32) {
	var virtualAddrEnd: UInt64 = virtualAddr + pageCount * pageSize

	var vAddress: UInt64 = virtualAddr
	var pAddress: UInt64 = physicalAddr

	while (vAddress < virtualAddrEnd) {
		map_p4huge(pml4entries, vAddress, pAddress)

		vAddress += pageSize * 512
		pAddress += pageSize * 512
	}
}

const efi.EFI_MEMORY_DESCRIPTOR *getNextDescriptor(const efi.descriptor: Pointer<EFI_MEMORY_DESCRIPTOR>,
														   descriptorSize: UInt64) {
	const UInt8 *desc = ((const UInt8 *)descriptor) + descriptorSize
	return (const efi.EFI_MEMORY_DESCRIPTOR *)desc
}

UInt64 getRAMSize(EfiMemoryMap *memoryMap) {
	const efi.EFI_MEMORY_DESCRIPTOR *descriptor = memoryMap.memoryMap
	let descriptorSize: UInt64 = memoryMap.descriptorSize
	var numberOfPages: UInt64 = 0

	var startOfMemoryMap: UInt64 = (memoryMap.memoryMap as! UInt64)
	var endOfMemoryMap: UInt64 = startOfMemoryMap + memoryMap.memoryMapSize
	var offset: UInt64 = startOfMemoryMap

	while (offset < endOfMemoryMap) {
		numberOfPages += descriptor.NumberOfPages

		offset += descriptorSize
		descriptor = getNextDescriptor(descriptor, descriptorSize)
	}

	return numberOfPages * pageSize
}

// TODO this should be reconsidered (i.e. pointer fixups)
fun mapEfi(EfiMemoryMap *memoryMap) {
	serialPrintln("[paging] mapping efi")

	const efi.EFI_MEMORY_DESCRIPTOR *descriptor = memoryMap.memoryMap
	let descriptorSize: UInt64 = memoryMap.descriptorSize
	var sum: UInt64 = 0

	var startOfMemoryMap: UInt64 = (memoryMap.memoryMap as! UInt64)
	var endOfMemoryMap: UInt64 = startOfMemoryMap + memoryMap.memoryMapSize
	var offset: UInt64 = startOfMemoryMap

	while (offset < endOfMemoryMap) {
		if descriptor.Attribute & EFI_MEMORY_RUNTIME {
			// TODO pointer fixups
			// TODO should be RW or RO?
			// Note: in VirtualBox, PhysicalStart seems way beyound RAM bounds
			// Seems like fixup should be applied only if PhysicalStart < ramBytes?
			mapMemory(descriptor.PhysicalStart, descriptor.PhysicalStart, descriptor.NumberOfPages, 1)
			sum += descriptor.NumberOfPages
		}

		offset += descriptorSize
		descriptor = getNextDescriptor(descriptor, descriptorSize)
	}

	serialPrintf("[paging] efi mapped %u pages\n", sum)
}

#if false
UInt64 conventionalAllocate(EfiMemoryMap *memoryMap, pages: UInt32) {
	const efi.EFI_MEMORY_DESCRIPTOR *descriptor = memoryMap.memoryMap
	let descriptorSize: UInt64 = memoryMap.descriptorSize
	var result: UInt64 = 0

	var startOfMemoryMap: UInt64 = (memoryMap.memoryMap as! UInt64)
	var endOfMemoryMap: UInt64 = startOfMemoryMap + memoryMap.memoryMapSize
	var offset: UInt64 = startOfMemoryMap

	while offset < endOfMemoryMap {

		if ((descriptor.Type == efi.EfiConventionalMemory) && (descriptor.NumberOfPages >= (pages + 1))) {
			serialPrintf("[paging] success allocate %d pages\n", pages)
			result = ((descriptor.PhysicalStart / pageSize) * pageSize + pageSize)
		}

		offset += descriptorSize
		descriptor = getNextDescriptor(descriptor, descriptorSize)
	}

	return result
}
#end

var buffa[1]: UInt8 = {0}

UInt64 conventionalAllocateLargest(EfiMemoryMap *memoryMap) {
	const efi.EFI_MEMORY_DESCRIPTOR *descriptor = memoryMap.memoryMap
	let descriptorSize: UInt64 = memoryMap.descriptorSize
	var result: UInt64 = 0
	var largestPages: UInt64 = 0

	var startOfMemoryMap: UInt64 = (memoryMap.memoryMap as! UInt64)
	var endOfMemoryMap: UInt64 = startOfMemoryMap + memoryMap.memoryMapSize
	var offset: UInt64 = startOfMemoryMap

	while offset < endOfMemoryMap {
		// Note: > not >= cause we should have some extra space next to this
		if descriptor.Type == efi.EfiConventionalMemory, descriptor.NumberOfPages > largestPages {
			largestPages = descriptor.NumberOfPages
			result = descriptor.PhysicalStart
		}

		offset += descriptorSize
		descriptor = getNextDescriptor(descriptor, descriptorSize)
	}

	serialPrintf("[paging] conventionalAllocateLargest is %u bytes, %d pages\n",
				 (uint32_t)(largestPages * pageSize), largestPages)

	return result
}

var conventionalOffset: UInt64 = 0;
fun conventionalAllocateNext(bytes: UInt64): UInt64 {
	let result = conventionalOffset
	let pages = bytes / pageSize // Math.floor
	conventionalOffset += pages * pageSize
	if (bytes - (pages * pageSize)) > 0 {
		conventionalOffset += pageSize
	}
	return result
}

fun mapFramebuffer(fb: ConstPointer<Framebuffer>) {
	let framebufferBase = fb.base
	// TODO map with hugepages!!!
	// TODO map ALL with EPIC pages?
	mapMemory(framebufferStart, (framebufferBase as! UInt64), fb.size / pageSize + 1, 1)
}
