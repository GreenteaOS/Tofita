// The Tofita Kernel
// Copyright (C) 2022 Oleg Petrenko
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Lesser General Public License as published by
// the Free Software Foundation, version 3 of the License.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Lesser General Public License for more details.
//
// You should have received a copy of the GNU Lesser General Public License
// along with this program.  If not, see <https://www.gnu.org/licenses/>.

let addressBits = 12

@struct @packed @sizeOf(8)
class LinearAddress {
	@bits(12) var offset: UInt64
	@bits(9) var pt: UInt64
	@bits(9) var pd: UInt64
	@bits(9) var pdpt: UInt64
	@bits(9) var pml4: UInt64
	@bits(16) var reserved: UInt64
}

// `512 entries * 8 bytes = 4 KiB`
// PML4 size = PDP size = PD size = PT size
//
// Since we are using 64 bit mode, the entry size
// is 64 bits.

let pageTableSize = 512

// Entry in a page table
@struct @packed @sizeOf(8)
class PageEntry {
	// Is the page present in physical memory?
	@bits(1) var present: UInt64

	// Pages are read-only by default
	@bits(1) var writeAllowed: UInt64

	// Pages are only accessible by supervisor by default
	@bits(1) var accessibleByAll: UInt64

	// Write through abilities of the page
	@bits(1) var writeThrough: UInt64

	// If set, the page will not be cached
	@bits(1) var cacheDisabled: UInt64

	// Was the page accessed?
	@bits(1) var accessed: UInt64

	// Has the page been written to? Only applicable for PTE.
	@bits(1) var dirty: UInt64

	// Page size by default is small, enabling this bit makes it bigger. Only applicable for PTE
	@bits(1) var largePage: UInt64

	// Prevent the translations cache from updating
	// the address in cache if CR3 is reset.
	//
	// Page global enable bit in CR4 has to be set to
	// enable this feature.
	//
	// Only applicable for PTE
	@bits(1) var global: UInt64

	// Not used by the processor
	@bits(3) var metadata: UInt64

	// Physical address of the child table/page
	@bits(40) var address: UInt64

	// Not used by the processor
	@bits(7) var metadata2: UInt64

	// Only applicable for PTE
	@bits(4) var protectionKey: UInt64

	// Disable execution of code from this page
	@bits(1) var disableExecution: UInt64
}

var pml4entries: PageEntry = null

fun allocatePage() {
	// TODO bounds check
	return conventionalAllocateNext(pageSize)
}

LinearAddress getLinearAddress(address: UInt64) {
	return *((LinearAddress *)Pointer.of(address))
}

fun initializePage(entry: PageEntry, address: UInt64, writeAllowed: UInt8) {
	entry.address = address >> addressBits
	entry.present = 1
	entry.writeAllowed = (writeAllowed == 1) ? 1 : 0
	entry.accessibleByAll = 1 // TODO
}

fun initializePageHuge(entry: PageEntry, address: UInt64) {
	entry.address = address >> addressBits
	entry.present = 1
	entry.writeAllowed = 1
	entry.largePage = 1
	entry.accessibleByAll = 1 // TODO
}

Void *getPage(table: Pointer<PageEntry>, entryId: UInt64, writeAllowed: UInt8) {
	var entry: Pointer<PageEntry> = Pointer.of(table[entryId])

		return (Void *)(entry.address << addressBits)
	if entry.present == 1 {
	} else {
		var newPage: Pointer<Void> = allocatePage()
		initializePage(entry, (newPage as! UInt64), writeAllowed)
		return newPage
	}
}

fun map_pt(PageEntry pt[], virtualAddr: UInt64, physicalAddr: UInt64, writeAllowed: UInt8) {
	var entry: Pointer<PageEntry> = Pointer.of(pt[getLinearAddress(virtualAddr).pt])
	initializePage(entry, physicalAddr, writeAllowed)
}

fun map_p2huge(PageEntry pd[], virtualAddr: UInt64, physicalAddr: UInt64) {
	var entry: Pointer<PageEntry> = Pointer.of(pd[getLinearAddress(virtualAddr).pd])
	initializePageHuge(entry, physicalAddr)
}

#define createMapping(fromTable, toTable)                                                                    \
	Void map_##fromTable(PageEntry fromTable[], virtualAddr: UInt64, physicalAddr: UInt64,          \
								UInt8 writeAllowed) {                                                      \
		var toTable: Pointer<Void> = getPage(fromTable, getLinearAddress(virtualAddr).fromTable, writeAllowed)           \
		map_##toTable((PageEntry *)toTable, virtualAddr, physicalAddr, writeAllowed)                        \
	}

createMapping(pd, pt)
createMapping(pdpt, pd)
createMapping(pml4, pdpt)

#define createHugeMapping(name, fromTable, calls, toTable)                                                   \
	Void map_##name(PageEntry fromTable[], virtualAddr: UInt64, physicalAddr: UInt64) {             \
		var toTable: Pointer<Void> = getPage(fromTable, getLinearAddress(virtualAddr).fromTable, 1)                      \
		map_##calls((PageEntry *)toTable, virtualAddr, physicalAddr)                                        \
	}

createHugeMapping(p3huge, pdpt, p2huge, pd)
createHugeMapping(p4huge, pml4, p3huge, pdpt)

fun mapMemory(virtualAddr: UInt64, physicalAddr: UInt64, pageCount: UInt32, writeAllowed: UInt8) {
	serialPrintln("[paging] mapping memory range")

	var virtualAddrEnd: UInt64 = virtualAddr + pageCount * pageSize

	var vAddress: UInt64 = virtualAddr
	var pAddress: UInt64 = physicalAddr

	serialPrintf("[paging.range] bytes = %d or %d\n", virtualAddrEnd - virtualAddr, pageCount * pageSize)

	serialPrint("[paging.range] virtual address = ")
	serialPrintHex(virtualAddr as! UInt64)
	serialPrint("\n")

	serialPrint("[paging.range] physical address = ")
	serialPrintHex(physicalAddr as! UInt64))
	serialPrint("\n")

	serialPrint("[paging.range] page count = ")
	serialPrintHex(pageCount as! UInt64)
	serialPrint("\n")

	serialPrint("[paging.range] virtual address end = ")
	serialPrintHex(virtualAddrEnd as! UInt64)
	serialPrint("\n")

	while vAddress < virtualAddrEnd {
		mapPML4(pml4entries, vAddress, pAddress, writeAllowed)

		vAddress += pageSize
		pAddress += pageSize
	}
}

fun mapMemoryHuge(virtualAddr: UInt64, physicalAddr: UInt64, pageCount: UInt32) {
	var virtualAddrEnd: UInt64 = virtualAddr + pageCount * pageSize

	var vAddress: UInt64 = virtualAddr
	var pAddress: UInt64 = physicalAddr

	while vAddress < virtualAddrEnd {
		mapP4Huge(pml4entries, vAddress, pAddress)

		vAddress += pageSize * 512
		pAddress += pageSize * 512
	}
}

const efi.EFI_MEMORY_DESCRIPTOR *getNextDescriptor(const efi.descriptor: Pointer<EFI_MEMORY_DESCRIPTOR>,
														   descriptorSize: UInt64) {
	const UInt8 *desc = ((const UInt8 *)descriptor) + descriptorSize
	return (const efi.EFI_MEMORY_DESCRIPTOR *)desc
}

UInt64 getRAMSize(EfiMemoryMap *memoryMap) {
	const efi.EFI_MEMORY_DESCRIPTOR *descriptor = memoryMap.memoryMap
	let descriptorSize: UInt64 = memoryMap.descriptorSize
	var numberOfPages: UInt64 = 0

	var startOfMemoryMap: UInt64 = (memoryMap.memoryMap as! UInt64)
	var endOfMemoryMap: UInt64 = startOfMemoryMap + memoryMap.memoryMapSize
	var offset: UInt64 = startOfMemoryMap

	while offset < endOfMemoryMap {
		numberOfPages += descriptor.NumberOfPages

		offset += descriptorSize
		descriptor = getNextDescriptor(descriptor, descriptorSize)
	}

	return numberOfPages * pageSize
}

// TODO this should be reconsidered (i.e. pointer fixups)
fun mapEfi(memoryMap: EfiMemoryMap) {
	serialPrintln("[paging] mapping efi")

	const EFI_MEMORY_DESCRIPTOR *descriptor = memoryMap.memoryMap
	let descriptorSize: UInt64 = memoryMap.descriptorSize
	var sum: UInt64 = 0

	var startOfMemoryMap: UInt64 = (memoryMap.memoryMap as! UInt64)
	var endOfMemoryMap: UInt64 = startOfMemoryMap + memoryMap.memoryMapSize
	var offset: UInt64 = startOfMemoryMap

	while offset < endOfMemoryMap {
		if descriptor.attribute & EFI_MEMORY_RUNTIME {
			// TODO pointer fixups
			// TODO should be RW or RO?
			// Note: in VirtualBox, PhysicalStart seems way beyound RAM bounds
			// Seems like fixup should be applied only if PhysicalStart < ramBytes?
			mapMemory(descriptor.PhysicalStart, descriptor.PhysicalStart, descriptor.NumberOfPages, 1)
			sum += descriptor.numberOfPages
		}

		offset += descriptorSize
		descriptor = getNextDescriptor(descriptor, descriptorSize)
	}

	serialPrintf("[paging] efi mapped %u pages\n", sum)
}

var buffa[1]: UInt8 = {0}

UInt64 conventionalAllocateLargest(EfiMemoryMap *memoryMap) {
	const efi.EFI_MEMORY_DESCRIPTOR *descriptor = memoryMap.memoryMap
	let descriptorSize: UInt64 = memoryMap.descriptorSize
	var result: UInt64 = 0
	var largestPages: UInt64 = 0

	var startOfMemoryMap: UInt64 = (memoryMap.memoryMap as! UInt64)
	var endOfMemoryMap: UInt64 = startOfMemoryMap + memoryMap.memoryMapSize
	var offset: UInt64 = startOfMemoryMap

	while offset < endOfMemoryMap {
		// Note: > not >= cause we should have some extra space next to this
		if descriptor.Type == EfiConventionalMemory, descriptor.NumberOfPages > largestPages {
			largestPages = descriptor.NumberOfPages
			result = descriptor.PhysicalStart
		}

		offset += descriptorSize
		descriptor = getNextDescriptor(descriptor, descriptorSize)
	}

	serialPrintf("[paging] conventionalAllocateLargest is %u bytes, %d pages\n",
				 (uint32_t)(largestPages * pageSize), largestPages)

	return result
}

var conventionalOffset: UInt64 = 0
fun conventionalAllocateNext(bytes: UInt64): UInt64 {
	let result = conventionalOffset
	let pages = bytes / pageSize // Math.floor
	conventionalOffset += pages * pageSize
	if (bytes - (pages * pageSize)) > 0 {
		conventionalOffset += pageSize
	}
	return result
}

fun mapFramebuffer(fb: ConstPointer<Framebuffer>) {
	let framebufferBase = fb.base
	// TODO map with hugepages!!!
	// TODO map ALL with EPIC pages?
	mapMemory(framebufferStart, (framebufferBase as! UInt64), fb.size / pageSize + 1, 1)
}
