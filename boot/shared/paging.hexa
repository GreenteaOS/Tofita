// The Tofita Kernel
// Copyright (C) 2020  Oleg Petrenko
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Lesser General Public License as published by
// the Free Software Foundation, version 3 of the License.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Lesser General Public License for more details.
//
// You should have received a copy of the GNU Lesser General Public License
// along with this program.  If not, see <https://www.gnu.org/licenses/>.



let ADDRESS_BITS = 12
#define PAGE_ALIGNED @aligned(PAGE_SIZE)
#define PACKED @packed

@struct
class LinearAddress {
	unsigned long long offset : 12
	unsigned long long pt : 9
	unsigned long long pd : 9
	unsigned long long pdpt : 9
	unsigned long long pml4 : 9
	unsigned long long reserved : 16
} @packed

_Static_assert(sizeof(uint64_t) == 8, "UInt64 has to have 64 bits")
_Static_assert(sizeof(paging.LinearAddress) == 8, "linear address has to have 64 bits")

// 512 entries * 8 bytes = 4 KiB
// PML4 size = PDP size = PD size = PT size
//
// Since we are using 64 bit mode, the entry size
// is 64 bits.

let PAGE_TABLE_SIZE = 512

// Entry in a page table
@struct
class {
	// Is the page present in physical memory?
	unsigned long long present : 1

	// Pages are read-only by default
	unsigned long long writeAllowed : 1

	// Pages are only accessible by supervisor by default
	unsigned long long accessibleByAll : 1

	// Write through abilities of the page
	unsigned long long writeThrough : 1

	// If set, the page will not be cached
	unsigned long long cacheDisabled : 1

	// Was the page accessed?
	unsigned long long accessed : 1

	// Has the page been written to? Only applicable for PTE.
	unsigned long long dirty : 1

	// Page size by default is small, enabling this bit makes it bigger. Only applicable for PTE
	unsigned long long largePage : 1

	// Prevent the translations cache from updating
	// the address in cache if CR3 is reset.
	//
	// Page global enable bit in CR4 has to be set to
	// enable this feature.
	//
	// Only applicable for PTE
	unsigned long long global : 1

	// Not used by the processor
	unsigned long long metadata : 3

	// Physical address of the child table/page
	unsigned long long address : 40

	// Not used by the processor
	unsigned long long metadata2 : 7

	// Only applicable for PTE
	unsigned long long protectionKey : 4

	// Disable execution of code from this page
	unsigned long long disableExecution : 1
} PACKED PageEntry

_Static_assert(sizeof(PageEntry) == sizeof(uint64_t), "page entry has to be 64 bits")

static PageEntry *pml4entries PAGE_ALIGNED = null

UInt64 conventionalAllocateNext(bytes: UInt64)
static Void *allocatePage() {
	// TODO bounds check
	return (Void *)conventionalAllocateNext(PAGE_SIZE)
}

static LinearAddress getLinearAddress(address: UInt64) {
	return *((LinearAddress *)&address)
}

static fun initializePage(PageEntry *entry, address: UInt64, writeAllowed: UInt8) {
	entry.address = address >> ADDRESS_BITS
	entry.present = 1
	entry.writeAllowed = (writeAllowed == 1) ? 1 : 0
	entry.accessibleByAll = 1 // TODO
}

static fun initializePageHuge(PageEntry *entry, address: UInt64) {
	entry.address = address >> ADDRESS_BITS
	entry.present = 1
	entry.writeAllowed = 1
	entry.largePage = 1
	entry.accessibleByAll = 1 // TODO
}

static Void *getPage(PageEntry *table, entryId: UInt64, writeAllowed: UInt8) {
	PageEntry *entry = &table[entryId]

	if (entry.present == 1) {
		return (Void *)(entry.address << ADDRESS_BITS)
	} else {
		Void *newPage = allocatePage()
		initializePage(entry, (uint64_t)newPage, writeAllowed)
		return newPage
	}
}

fun map_pt(PageEntry pt[], virtualAddr: UInt64, physicalAddr: UInt64, writeAllowed: UInt8) {
	PageEntry *entry = &pt[getLinearAddress(virtualAddr).pt]
	initializePage(entry, physicalAddr, writeAllowed)
}

fun map_p2huge(PageEntry pd[], virtualAddr: UInt64, physicalAddr: UInt64) {
	PageEntry *entry = &pd[getLinearAddress(virtualAddr).pd]
	initializePageHuge(entry, physicalAddr)
}

#define createMapping(fromTable, toTable)                                                                    \
	static Void map_##fromTable(PageEntry fromTable[], virtualAddr: UInt64, physicalAddr: UInt64,          \
								UInt8 writeAllowed) {                                                      \
		Void *toTable = getPage(fromTable, getLinearAddress(virtualAddr).fromTable, writeAllowed)           \
		map_##toTable((PageEntry *)toTable, virtualAddr, physicalAddr, writeAllowed)                        \
	}

createMapping(pd, pt)
createMapping(pdpt, pd)
createMapping(pml4, pdpt)

#define createHugeMapping(name, fromTable, calls, toTable)                                                   \
	static Void map_##name(PageEntry fromTable[], virtualAddr: UInt64, physicalAddr: UInt64) {             \
		Void *toTable = getPage(fromTable, getLinearAddress(virtualAddr).fromTable, 1)                      \
		map_##calls((PageEntry *)toTable, virtualAddr, physicalAddr)                                        \
	}

createHugeMapping(p3huge, pdpt, p2huge, pd)
createHugeMapping(p4huge, pml4, p3huge, pdpt)

fun mapMemory(UInt64 virtualAddr, physicalAddr: UInt64, pageCount: UInt32, writeAllowed: UInt8) {
	serialPrintln("[paging] mapping memory range")

	var virtualAddrEnd: UInt64 = virtualAddr + pageCount * PAGE_SIZE

	var vAddress: UInt64 = virtualAddr
	var pAddress: UInt64 = physicalAddr

	serialPrintf("[paging.range] bytes = %d or %d\n", virtualAddrEnd - virtualAddr, pageCount * PAGE_SIZE)

	serialPrint("[paging.range] virtual address = ")
	serialPrintHex((uint64_t)(virtualAddr))
	serialPrint("\n")

	serialPrint("[paging.range] physical address = ")
	serialPrintHex((uint64_t)(physicalAddr))
	serialPrint("\n")

	serialPrint("[paging.range] page count = ")
	serialPrintHex((uint64_t)(pageCount))
	serialPrint("\n")

	serialPrint("[paging.range] virtual address end = ")
	serialPrintHex((uint64_t)(virtualAddrEnd))
	serialPrint("\n")

	while (vAddress < virtualAddrEnd) {
		map_pml4(pml4entries, vAddress, pAddress, writeAllowed)

		vAddress += PAGE_SIZE
		pAddress += PAGE_SIZE
	}
}

fun mapMemoryHuge(UInt64 virtualAddr, physicalAddr: UInt64, pageCount: UInt32) {
	var virtualAddrEnd: UInt64 = virtualAddr + pageCount * PAGE_SIZE

	var vAddress: UInt64 = virtualAddr
	var pAddress: UInt64 = physicalAddr

	while (vAddress < virtualAddrEnd) {
		map_p4huge(pml4entries, vAddress, pAddress)

		vAddress += PAGE_SIZE * 512
		pAddress += PAGE_SIZE * 512
	}
}

static const efi.EFI_MEMORY_DESCRIPTOR *getNextDescriptor(const efi.EFI_MEMORY_DESCRIPTOR *descriptor,
														   UInt64 descriptorSize) {
	const UInt8 *desc = ((const UInt8 *)descriptor) + descriptorSize
	return (const efi.EFI_MEMORY_DESCRIPTOR *)desc
}

UInt64 getRAMSize(EfiMemoryMap *memoryMap) {
	const efi.EFI_MEMORY_DESCRIPTOR *descriptor = memoryMap.memoryMap
	let descriptorSize: UInt64 = memoryMap.descriptorSize
	var numberOfPages: UInt64 = 0

	var startOfMemoryMap: UInt64 = (uint64_t)memoryMap.memoryMap
	var endOfMemoryMap: UInt64 = startOfMemoryMap + memoryMap.memoryMapSize
	var offset: UInt64 = startOfMemoryMap

	while (offset < endOfMemoryMap) {
		numberOfPages += descriptor.NumberOfPages

		offset += descriptorSize
		descriptor = getNextDescriptor(descriptor, descriptorSize)
	}

	return numberOfPages * PAGE_SIZE
}

// TODO this should be reconsidered (i.e. pointer fixups)
fun mapEfi(EfiMemoryMap *memoryMap) {
	serialPrintln("[paging] mapping efi")

	const efi.EFI_MEMORY_DESCRIPTOR *descriptor = memoryMap.memoryMap
	let descriptorSize: UInt64 = memoryMap.descriptorSize
	var sum: UInt64 = 0

	var startOfMemoryMap: UInt64 = (uint64_t)memoryMap.memoryMap
	var endOfMemoryMap: UInt64 = startOfMemoryMap + memoryMap.memoryMapSize
	var offset: UInt64 = startOfMemoryMap

	while (offset < endOfMemoryMap) {
		if (descriptor.Attribute & EFI_MEMORY_RUNTIME) {
			// TODO pointer fixups
			// TODO should be RW or RO?
			// Note: in VirtualBox, PhysicalStart seems way beyound RAM bounds
			// Seems like fixup should be applied only if PhysicalStart < ramBytes?
			mapMemory(descriptor.PhysicalStart, descriptor.PhysicalStart, descriptor.NumberOfPages, 1)
			sum += descriptor.NumberOfPages
		}

		offset += descriptorSize
		descriptor = getNextDescriptor(descriptor, descriptorSize)
	}

	serialPrintf("[paging] efi mapped %u pages\n", sum)
}

#if false
UInt64 conventionalAllocate(EfiMemoryMap *memoryMap, pages: UInt32) {
	const efi.EFI_MEMORY_DESCRIPTOR *descriptor = memoryMap.memoryMap
	let descriptorSize: UInt64 = memoryMap.descriptorSize
	var result: UInt64 = 0

	var startOfMemoryMap: UInt64 = (uint64_t)memoryMap.memoryMap
	var endOfMemoryMap: UInt64 = startOfMemoryMap + memoryMap.memoryMapSize
	var offset: UInt64 = startOfMemoryMap

	while (offset < endOfMemoryMap) {

		if ((descriptor.Type == efi.EfiConventionalMemory) && (descriptor.NumberOfPages >= (pages + 1))) {
			serialPrintf("[paging] success allocate %d pages\n", pages)
			result = ((descriptor.PhysicalStart / PAGE_SIZE) * PAGE_SIZE + PAGE_SIZE)
		}

		offset += descriptorSize
		descriptor = getNextDescriptor(descriptor, descriptorSize)
	}

	return result
}
#end

var buffa[1]: UInt8 = {0}

UInt64 conventionalAllocateLargest(EfiMemoryMap *memoryMap) {
	const efi.EFI_MEMORY_DESCRIPTOR *descriptor = memoryMap.memoryMap
	let descriptorSize: UInt64 = memoryMap.descriptorSize
	var result: UInt64 = 0
	var largestPages: UInt64 = 0

	var startOfMemoryMap: UInt64 = (uint64_t)memoryMap.memoryMap
	var endOfMemoryMap: UInt64 = startOfMemoryMap + memoryMap.memoryMapSize
	var offset: UInt64 = startOfMemoryMap

	while (offset < endOfMemoryMap) {
		// Note: > not >= cause we should have some extra space next to this
		if ((descriptor.Type == efi.EfiConventionalMemory) && (descriptor.NumberOfPages > largestPages)) {
			largestPages = descriptor.NumberOfPages
			result = descriptor.PhysicalStart
		}

		offset += descriptorSize
		descriptor = getNextDescriptor(descriptor, descriptorSize)
	}

	serialPrintf("[paging] conventionalAllocateLargest is %u bytes, %d pages\n",
				 (uint32_t)(largestPages * PAGE_SIZE), largestPages)

	return result
}

UInt64 conventionalOffset
UInt64 conventionalAllocateNext(bytes: UInt64) {
	let result = conventionalOffset
	let pages = bytes / PAGE_SIZE // Math.floor
	conventionalOffset += pages * PAGE_SIZE
	if ((bytes - (pages * PAGE_SIZE)) > 0)
		conventionalOffset += PAGE_SIZE
	return result
}

fun mapFramebuffer(const Framebuffer *fb) {
	let framebufferBase = fb.base
	// TODO map with hugepages!!!
	// TODO map ALL with EPIC pages?
	mapMemory(FramebufferStart, (uint64_t)framebufferBase, fb.size / PAGE_SIZE + 1, 1)
}
