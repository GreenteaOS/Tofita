// The Tofita Kernel
// Copyright (C) 2022-2023 Oleh Petrenko
//
// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU Lesser General Public License as published by
// the Free Software Foundation, version 3 of the License.
//
// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU Lesser General Public License for more details.
//
// You should have received a copy of the GNU Lesser General Public License
// along with this program.  If not, see <https://www.gnu.org/licenses/>.

// AMD64 pages format and map/un-map internal implementation

/// Align to 4096
let addressBits = 12

// TODO remove this
@aligned(4096) var pml4entries ArrayPointer<ByValue<PageEntry>> = null as! ArrayPointer<ByValue<PageEntry>>

// TODO combine flags into bit mask
fun mapPage(entry PageEntry, address UInt64, writeAllowed Bool, accessibleByUser Bool) {
	entry.address = address >> addressBits
	entry.present = 1
	entry.writeAllowed = writeAllowed ? 1u8 : 0u8
	// TODO `accessibleByUser and (v is lower half)` as a last-resort measure
	entry.accessibleByUser = accessibleByUser ? 1u8 : 0u8
}

fun referOrCreatePage(table ArrayPointer<ByValue<PageEntry>>, entryId UInt64, writeAllowed Bool, accessibleByUser Bool) ArrayPointer<ByValue<PageEntry>> {
	let entry PageEntry = table[entryId].ref

	if entry.present == 1 {
		// TODO `.present` is not always means allocated
		return ((entry.address << addressBits) + (wholePhysicalStart as! UInt64)) as! ArrayPointer<ByValue<PageEntry>>
	} else {
		var newPage = PhysicalAllocator.allocateOnePagePreZeroed() as! ArrayPointer<ByValue<PageEntry>>
		if newPage != null {
			mapPage(entry, (newPage as! UInt64) - (wholePhysicalStart as! UInt64), writeAllowed, accessibleByUser)
			// TODO else error
		}
		return newPage
	}
}

/// allows to access separate address by page table levels
@struct @packed @sizeOf(8)
class PageAddressView {
	@bits(12) var offset UInt64 = 0 // TODO `let` and no `= 0`
	@bits(9) var pt UInt64 = 0
	@bits(9) var pd UInt64 = 0
	@bits(9) var pdpt UInt64 = 0
	@bits(9) var pml4 UInt64 = 0
	@bits(16) var padding UInt64 = 0
	// new () // TODO
}

fun getPageAddressView(virtualAddr UInt64) ByValue<PageAddressView> {
	return virtualAddr as! ByValue<PageAddressView>
}

// TODO handle huge pages returned from bootloader
fun mapPT(pt ArrayPointer<ByValue<PageEntry>>, virtualAddr UInt64, physicalAddr UInt64, writeAllowed Bool, accessibleByUser Bool) {
	let view = getPageAddressView(virtualAddr)
	let entry PageEntry = pt[view.ref.pt].ref
	mapPage(entry, physicalAddr, writeAllowed, accessibleByUser)
}

fun mapPD(pd ArrayPointer<ByValue<PageEntry>>, virtualAddr UInt64, physicalAddr UInt64,
							writeAllowed Bool, accessibleByUser Bool) Void {
	let view = getPageAddressView(virtualAddr)
	let pt = referOrCreatePage(pd, view.ref.pd, writeAllowed, accessibleByUser)
	mapPT(pt, virtualAddr, physicalAddr, writeAllowed, accessibleByUser)
}

fun mapPDPT(pdpt ArrayPointer<ByValue<PageEntry>>, virtualAddr UInt64, physicalAddr UInt64,
							writeAllowed Bool, accessibleByUser Bool) Void {
	let view = getPageAddressView(virtualAddr)
	let pd = referOrCreatePage(pdpt, view.ref.pdpt, writeAllowed, accessibleByUser)
	mapPD(pd, virtualAddr, physicalAddr, writeAllowed, accessibleByUser)
}

// TODO `private` all those functions
fun mapPML4(pml4 ArrayPointer<ByValue<PageEntry>>, virtualAddr UInt64, physicalAddr UInt64,
							writeAllowed Bool, accessibleByUser Bool) Void {
	let view = getPageAddressView(virtualAddr)
	let pdpt = referOrCreatePage(pml4, view.ref.pml4, writeAllowed, accessibleByUser)
	mapPDPT(pdpt, virtualAddr, physicalAddr, writeAllowed, accessibleByUser)
	// TODO present = true must be set in reverse order to avoid race condition on un-cached access?
	// i.e. PT then PD then PDPT, currently it set PD present before PT
	// TODO inv pg asm to invalidate page cache
}

/// Returns actual physical address from virtual address
fun resolveAddr(pml4entries ArrayPointer<ByValue<PageEntry>>, virtualAddr UInt64) UInt64 {
	// TODO unsupported by C `let linear = getPageAddressView(virtualAddr).ref`
	let linear = getPageAddressView(virtualAddr)
	let linear = linear.ref // TODO better idea
	// pml4
	var entry = pml4entries[linear.pml4] as! ArrayPointer<ByValue<PageEntry>>
	if entry[0].ref.present == 0 {
		return physical_not_found
	}
	entry = ((entry[0].ref.address << addressBits) + (wholePhysicalStart as! UInt64)) as! ArrayPointer<ByValue<PageEntry>>
	// pdpt
	entry = entry[linear.pdpt] as! ArrayPointer<ByValue<PageEntry>>
	if entry[0].ref.present == 0 {
		return physical_not_found
	}
	// TODO handle mega pages
	entry = ((entry[0].ref.address << addressBits) + (wholePhysicalStart as! UInt64)) as! ArrayPointer<ByValue<PageEntry>>
	// pd
	entry = entry[linear.pd] as! ArrayPointer<ByValue<PageEntry>>
	if entry[0].ref.present == 0 {
		return physical_not_found
	}
	if entry[0].ref.largePage == 1 {
		return (entry[0].ref.address << addressBits) + linear.pt * 4096 + linear.offset
	}
	entry = ((entry[0].ref.address << addressBits) + (wholePhysicalStart as! UInt64)) as! ArrayPointer<ByValue<PageEntry>>
	// pt
	entry = entry[linear.pt] as! ArrayPointer<ByValue<PageEntry>>
	if entry[0].ref.present == 0 {
		return physical_not_found
	}
	return (entry[0].ref.address << addressBits) + linear.offset
}

// TODO return error code (as enum)
// TODO ^ hexa @mustCheckReturn for return values (like Golang)
// TODO ^ same clang warning
// TODO should accept usermode flag so shared memory and GDI buffers can be mapped out of user scope
// ^ probably should be other way for WoW processes
fun mapMemory(pml4entries ArrayPointer<ByValue<PageEntry>>, virtualAddr UInt64, physicalAddr UInt64, pageCount UInt32, accessibleByUser Bool) {
	serialPrintln("[paging] mapping memory range")

	var virtualAddrEnd UInt64 = virtualAddr + pageCount * pageSize

	var vAddress UInt64 = virtualAddr
	var pAddress UInt64 = physicalAddr

	serialPrintf("[paging.range] bytes = %d or %d\n", virtualAddrEnd - virtualAddr, pageCount * pageSize)

	serialPrint("[paging.range] virtual address = ")
	serialPrintHex(virtualAddr as! UInt64)
	serialPrint("\n")

	serialPrint("[paging.range] physical address = ")
	serialPrintHex(physicalAddr as! UInt64)
	serialPrint("\n")

	serialPrint("[paging.range] page count = ")
	serialPrintHex(pageCount as! UInt64)
	serialPrint("\n")

	serialPrint("[paging.range] virtual address end = ")
	serialPrintHex(virtualAddrEnd as! UInt64)
	serialPrint("\n")

	while vAddress < virtualAddrEnd {
		let writeAllowed = true // TODO
		mapPML4(pml4entries, vAddress, pAddress, writeAllowed, accessibleByUser)

		vAddress += pageSize
		pAddress += pageSize
	}

	// TODO rethink where to call this
	writeCr3((pml4entries as! UInt64) - (wholePhysicalStart as! UInt64))
}

// Unmaps pages without actually deallocating them, still deallocates unused page tables
// (page tables are managed only and only here anyway)
fun unmapMemory(pml4entries PageEntry, virtualAddr UInt64, pageCount UInt32) {
	// TODO do NOT unmap reserved pages, i.e. wholePhysicalStart and upper half region!
	// TODO probably special shared buffer allocator+mapper which uses linked list
	// way upper than wholePhysicalStart limit
	// TODO
}

// Completely unmaps and deallocates pages and page tables
fun unmapAndFreeMemory(pml4entries PageEntry, virtualAddr UInt64, pageCount UInt32) {
	// TODO resolve and dealloc pages
	unmapMemory(pml4entries, virtualAddr, pageCount)
}

// Upper half
// Just makes a copy of upper half's PML4,
// because it is always the same between processes
fun copyKernelMemoryMap(pml4source ArrayPointer<ByValue<PageEntry>>, pml4destination ArrayPointer<ByValue<PageEntry>>) {
	var i UInt32 = 255 // TODO validate that this is where upper half starts
	let pageTableEntriesCount = 512u64

	while i < pageTableEntriesCount {
		pml4destination[i] = pml4source[i]
		i++
	}
}

// TODO rename pml4 to virtualMemoryTable/Map newVirtualMemoryTable
// Creates new PML4 for new process
fun newCR3(pml4source ArrayPointer<ByValue<PageEntry>>) ArrayPointer<ByValue<PageEntry>> {
	var pml4result = PhysicalAllocator.allocateOnePagePreZeroed() as! ArrayPointer<ByValue<PageEntry>>
	if pml4result != null {
		copyKernelMemoryMap(pml4source, pml4result)
	}
	return pml4result
}

fun freeCR3(pml4 PageEntry) Void {
	// TODO deallocate full lower half (page.dest.arc--)
}

// Lower half
// TODO protection ring
// TODO lower half limit bounds check (it is less than upper starting range)
fun mapUserspaceMemory(pml4entries PageEntry, virtualAddr UInt64, physicalAddr UInt64,
							pageCount UInt32) {
	// TODO
}

// Same as VirtualAlloc
// TODO protection ring
// TODO respect WoW limits
// TODO VirtualAllocEx-like behavior (i.e. alloc for other processes, not only current)
// Decides automatically where to allocate
// if virtualAddr == 0
// Note: also allocates physical pages, i.e consumes extra memory
fun allocUserspaceMemory(pml4entries PageEntry, virtualAddr UInt64, pageCount UInt32) {
	// TODO
}

// TODO protection ring
// TODO respect WoW limits
// TODO VirtualAllocEx-like behavior (i.e. alloc for other processes, not only current)
// Decides automatically where to allocate
// if virtualAddr == 0 TODO separate func isAvailableUserMemoryRange and findAvailableUserMemoryRange
// Does NOT allocate physical memory for the requested range
// Returns 0 if requested pointer is occupied
// TODO rename to findUserMemory or findLowerMemory or findProcessMemory
fun findUserspaceMemory(
	pml4entries ArrayPointer<ByValue<PageEntry>>,
	virtualAddr UInt64,
	pageCount UInt32,
	limits AddressAwareness
) UInt64 {
	let upperBound = 0x7FFFFFFFFFFFu64

	// TODO return 0 if virtualAddr not page aligned (virtualAddr & ADDRESS_BITS != 0)

	// Yeah, brute force PML4 for now
	var pageFirstByte = virtualAddr == 0? 4096u64 : virtualAddr
	let cycle = virtualAddr == 0

	while cycle and pageFirstByte < upperBound {
		var found = 0u64
		let result = pageFirstByte
		// TODO create isMapped function
		while resolveAddr(pml4entries, pageFirstByte) == physical_not_found {
			found++
			if found == pageCount {
				return result
			}
			pageFirstByte += 4096
		}

		while resolveAddr(pml4entries, pageFirstByte) != physical_not_found {
			pageFirstByte += 4096
		}
	}

	return 0
}
